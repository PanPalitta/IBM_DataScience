{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Predicting Traffic Collision</h1>\n",
    "\n",
    "<h3 align =\"center\">Data Science Capstone Project</h3>\n",
    "\n",
    "<p style=\"text-align:center;\">Pantita Palittapongarnpim\n",
    "\n",
    "<p style=\"text-align:center;\">September 2020</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Road traffic is severely affected by car collisions, which in turn caused lost of productivity and, if happened regularly lower quality of life. Being able to predict collisions and its severity can thus help mitigating its effect by providing information to drivers to better plan their travel.\n",
    "\n",
    "In this project, we aim to predict the severity of traffic accidents from road condition. Although this project is a rudimentary modelling project, when coupled with a map, this type of tool can be used for planning travel routes and time in order to avoid run-in with traffic jams caused by collisions or even being in a collision ourselves. \n",
    "\n",
    "This project is most interesting as an extended feature to online map and navigation services, where drivers can visualize the prediction with geographical information. Companies providing navigational services maybe interested in this tool as a part of their algorithm to provide the best traveling route to drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data ##\n",
    "\n",
    "The data set we will be using is on one on __[collisions in Seattle area](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv)__ collected by SPD with metadata available __[here](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Metadata.pdf)__. The data consisted of 37 attributes and 194,673 entries. An example of the data imported as a dataframe is given in the \n",
    "figure below.\n",
    "\n",
    "![!](raw_data.png)\n",
    "\n",
    "\n",
    "The severity of the collisions ('SEVERITYCODE') are classified into 5 different level of severity: fatality (3), serious injury (2b), injury (2), prop damage (1) and unknown (0). The data also provides detail of the crash, including the type of collision, the parties involved and the number of people killed and injured. The circumstance around the collision is recorded in terms of the road condition, the weather, light condition, whether drugs or alcohol or speeding was involved\n",
    "\n",
    "Given this data set, the most straightforward model is to used the circumstances around the collisions as attributes to predict the severity of the collision, disregarding the parties involved and the type of collision in the first attempt of the model for simplicity. The relevant attributed are\n",
    "\n",
    "* 'INATTENTIONIND', indicating whether driver's inattention is involved;\n",
    "* 'UNDERINFL', indicating whether the driver's is under the influence of drugs or alcohol;\n",
    "* 'WEATHER', specify the weather condition;\n",
    "* 'ROADCOND', specify the road condition;\n",
    "* 'LIGHTCOND', specify the lighting condition;\n",
    "* 'SPEEDING', specify whether the driver was speeding.\n",
    "\n",
    "These data, as present in the example below, are categorial and are stored as strings. Hence, data pre-processing is required before models can be learned.\n",
    "![!](data_attri.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology ##\n",
    "\n",
    "The processing of data are splitted into two steps. The first step is data pre-processing and preparation, where we take the dataframe described in the Data section and turned the categorial data from strings to integer. We then split the dataframe into training and testing set using function from sklearn libary.\n",
    "\n",
    "The second step is then to learn the model. As we are mapping categorial attributes to categorial targets, we focus on classification as the approach for modelling this data set. In particular, we fit the data to four different models covered in this course: K nearest neighbour, decision tree, logistic regression, or support vector machine. We employ ones that are available from sklearn library to test out the performance of each model. We also explore the metaparameter to optimize the model for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Pre-processing ###\n",
    "\n",
    "In this part, we process the entries in the dataframe and making sure that all data are numericals befores splitting them into trainning and testing set.\n",
    "\n",
    "The first three columns -- 'INATTENTIONIND', 'UNDERINFL', 'SPEEDING' -- should by definition be binary data. However, the data given to us contain NaN an no 0 in both 'INATTENTIONIND' and 'SPEEDING', and so the entries are rectify to only 0 and 1. As for 'UNDERINFL', it appears that there are attempts made to distinguish between the influence of alcohol and drugs, which are not of interest to us. We are only interested if the driver is influenced at all. Therefore, the data are also retified to only show 0 for \"not influenced\" and 1 for \"influenced\".\n",
    "\n",
    "As for 'WEATHER', 'ROADCOND', 'LIGHTCOND', the data are in strings. The standard practice for changing categorial data to numerical data is to create dummies for each category found in each column. The dummy variable for the entry that is true is marked as 1 and 0 in other variables. However, in this case, this practice would create a large number of dummy variables as each column can be one of 12 categories. Instead, we group the categories according to how it affects the driving and label the group using an integer with 0 being \"unknown condition\", 1 being \"not effecting driving\" and so on. An example of the attribute dataframe is below.\n",
    "![!](data_process.png)\n",
    "\n",
    "The target dataframe is also checked for entries containing \"2b\" as it would need to be changed to other label. We found that the severity data only contains one of two code in {1,2}, as shown below.\n",
    "![!](y_visual.png)\n",
    "As such, we can go ahead and split the data into training and testing set, using the method from sklearn\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.3, random_state=4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data classification ###\n",
    "\n",
    "In this part, we describe the data classification methods that we use to build a model for predicting severity of traffic collision.\n",
    "\n",
    "As previously mention, we focus on four models used in this course: KNN, SVM, decision tree, and logistic regression. And we will test these models over their metaparameter. The metaparameter is selected based on [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index). Each of the model, tuned, are then compared based on their Jaccard index and [F1 score](https://en.wikipedia.org/wiki/F1_score). These metrics are computed using the methods from sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "In this section, we present the results from both the preliminary testing of the models by plotting the Jaccard indices. We then compare the models based on Jaccard indices and F1 scores using the best metaparameters determined from the preliminary tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Testing Metaparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 KNN ####\n",
    "\n",
    "The metaparameter to the KNN model is the number of the nearest neighbours, which we explore from k=[1,14]. We found that the Jaccard index decreases from k=10 onwards, and so we did not attempt to explore beyond k=14. Within this range, k=2 provides the highest Jaccard index at testing accuracy of 0.691158.\n",
    "![!](KNN_meta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Decision Tree\n",
    "\n",
    "For the decision tree model, the metaparameter we are validating is the tree depth. We explore k=[2,24] and found that the highest Jaccard index comes from k=3 with testing accuracy of 0.703623.\n",
    "\n",
    "![!](dec_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 SVM\n",
    "\n",
    "The metaparameter for SVM is the kernels. There are many that can be selected from the library but we select the four basic one: linear, polynomial, sigmoid, and radial basis function (rbf). We found that with the exception of sigmoid, the other three kernels deliver the same value of Jaccard index. Hence, we choose linear kernel as our SVM kernel.\n",
    "\n",
    "|Kernel | Jaccard index     |\n",
    "|-------|-------------------|\n",
    "|linear | 0.7034519365775145|\n",
    "|poly   | 0.7034519365775145|\n",
    "|sigmoid| 0.6913975548782576|\n",
    "|rbf    | 0.7034519365775145|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Logistic Regression\n",
    "\n",
    "The metaparameter for logistic regression is the c parameter, which we vary from c=[0.1,38.1] to explore the parameter space. However, we discover that all the parameter deliver the same Jaccard index of 0.702921. Therefore, we choose c=0.1 for our model.\n",
    "\n",
    "![!](lr_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparing Classification Models\n",
    "\n",
    "In this section, we present the Jaccard index and F1 score from the models with tuned metaparamters in order to determine the best classifier for predicting collision severity. The results is in the table below.\n",
    "\n",
    "|Algorithm|Jaccard| \tF1 score|\n",
    "|---------|-------|-----------|\n",
    "| \tKNN |\t0.691158| \t0.588601|\n",
    "| Decision Tree |\t0.703452 |\t0.580990|\n",
    "|SVM \t| 0.703452 |\t0.580990|\n",
    "|Logistic Regression| \t0.702921 |\t0.581460|\n",
    "\n",
    "Here, we see that the highest Jaccard indices are those of the decision tree and SVM, whereas the highest F1 score comes from KNN. Neither of the metric provides values higher than 0.704 with F1 score being significantly lower than the Jaccard index for all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion\n",
    "\n",
    "In this part, we discuss the results found in the previous section, mainly in the comparison between classifiers. We have seen that the Jaccard index, which indicates the overlap between the test data and the prediction from the model, is delivered at 70% for all models, likely indicating that this is the highest accuracy we can obtain provided the format of data that we use.\n",
    "\n",
    "The F1 scores, however, are much lower. The reason is that the F1 score not only take into account the accurate predictions but also the false negative and false positive predictions, which reduces the score as we know we only have 70%-accurate classifiers to begin with. One direction to explore is to use F1 score as the metric for choosing the metaparameter instead of the Jaccard model, which may result in a different set of parameters entirely.\n",
    "\n",
    "It is also prudent to ask if the data set and attributes we use are suitable for perdicting the severity of collision. As the KNN model can get to these accuracy values with 2 neighbors and for decision tree at the depth of 3, there is a likely chance that we include too many attributes, which can confuse the learning process and reduce the quality of the classifier. There is also a question of whether including some negative examples, i.e., the cases with no crash, could help the classifier improves its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this project, we aim to predict the severity of traffic collision using classification techniques. By learning form collision data collection from Seattle area, we test classifiers based on KNN, decision tree, SVM, and logistic regession and found that we are able to obtain the Jaccard index of 0.7 and F1 score 0.58 for these models with variation in the values on the third digit. As the prediction accuracy are still low, further investigation into ways to improve prediction is needed from expanding the data types to change in objective function so that the classifier can be used for planning future travels to avoid traffic jams and collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
